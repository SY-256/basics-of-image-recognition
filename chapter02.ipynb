{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdDDm5OUGA5wedo1SOn2zL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SY-256/basics-of-image-recognition/blob/main/chapter02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7a-75IXHW-0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 全結合層\n",
        "class FullyConnectedLayer:\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        self.weights = np.random.randn(input_dim, output_dim) # 標準正規分布に従う乱数で重み作成\n",
        "        self.bias = np.zeros(output_dim) # バイアスは0で初期化\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        return np.dot(X, self.weights) + self.bias # X * W.T + bias\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"grad_output: ネットワーク上層から伝播してきた誤差勾配情報\"\"\"\n",
        "        grad_input = np.dot(grad_output, self.weights.T)\n",
        "        self.grad_weights = np.dot(self.X.T, grad_output)\n",
        "        # バイアスの逆伝播ではミニバッチサイズ分の勾配を合算して求める\n",
        "        self.grad_bias = np.sum(grad_output, axis=0)\n",
        "        return grad_input\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "        self.weights -= learning_rate * self.grad_weights\n",
        "        self.bias -= learning_rate * self.grad_bias\n"
      ],
      "metadata": {
        "id": "UrWNd3zEHqaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ReLUの実装\n",
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input = input_data\n",
        "        return np.maximum(0, input_data)\n",
        "\n",
        "    def backward(self, d_output):\n",
        "        grad_input = d_output.copy()\n",
        "        grad_input[self.input < 0] = 0\n",
        "        return grad_input"
      ],
      "metadata": {
        "id": "2EBNoek1JnqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 損失関数の実装（Softmax, CrossEntorpy）\n",
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, input_data, axis=-1):\n",
        "        exps = np.exp(input_data - np.max(input_data, axis=axis, keepdims=True))\n",
        "        output = exps / np.sum(exps, axis=axis, keepdims=True)\n",
        "        return output\n",
        "\n",
        "class CrossEntorpyLoss_With_Softmax:\n",
        "    def __init__(self):\n",
        "        self.sotfmax = Softmax()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        self.y = y\n",
        "        self.y_hat = self.sotfmax.forward(x)\n",
        "        return -np.sum(np.log(self.y_hat[np.argmax(self.y_hat.shape[0]), y] + 1e-8)) / self.y_hat.shape[0]\n",
        "\n",
        "\n",
        "    def backward(self):\n",
        "        batch_size = self.y.shape[0]\n",
        "        dx = self.y_hat.copy()\n",
        "        dx[np.arange(batch_size), self.y] -= 1\n",
        "        dx = dx / batch_size\n",
        "        return dx"
      ],
      "metadata": {
        "id": "HcUmTmEVKCLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 2層MLP\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        self.layers = [\n",
        "            FullyConnectedLayer(input_dim, hidden_dim),\n",
        "            ReLU(),\n",
        "            FullyConnectedLayer(hidden_dim, output_dim),\n",
        "        ]\n",
        "\n",
        "    def forward(self, X):\n",
        "        for layer in self.layers:\n",
        "            X = layer.forward(X)\n",
        "        return X\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        for layer in reversed(self.layers):\n",
        "            grad_output = layer.backward(grad_output)\n",
        "        return grad_output\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, FullyConnectedLayer):\n",
        "                layer.update(learning_rate)\n"
      ],
      "metadata": {
        "id": "yP6X5NwqOLhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 画像分類の実装\n",
        "\n",
        "mnist_data = MNIST('./mnist', train=True, download=True, transform=transforms.ToTensor())\n",
        "data_loader = DataLoader(mnist_data, batch_size=256, shuffle=True)\n",
        "\n",
        "\n",
        "num_epoch = 50\n",
        "learning_rate = 0.05\n",
        "\n",
        "# MLPクラスの生成\n",
        "mlp = MLP(784, 128, 10)\n",
        "\n",
        "criterion = CrossEntorpyLoss_With_Softmax()\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    loss = 0.0\n",
        "    accuracy = 0.0\n",
        "    for i, (X_mini, y_mini) in enumerate(data_loader):\n",
        "        X_mini = X_mini.view(X_mini.size(0), -1).numpy()\n",
        "        y_mini = y_mini.numpy()\n",
        "        y_pred = mlp.forward(X_mini)\n",
        "        loss += criterion.forward(y_pred, y_mini)\n",
        "\n",
        "        # backward\n",
        "        grad_output = criterion.backward()\n",
        "        mlp.backward(grad_output)\n",
        "\n",
        "        # update the weight parameters\n",
        "        mlp.update(learning_rate)\n",
        "\n",
        "        # Caluculate Accuracy\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "        accuracy += np.mean(y_mini == y_pred) * 100\n",
        "\n",
        "    loss /= len(data_loader)\n",
        "    accuracy /= len(data_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epoch}], Loss: {loss}, Accuracy: {accuracy}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftCmbcdG0ToL",
        "outputId": "939309a2-db30-46ae-c514-ec6d82890c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 16.020813101624107, Accuracy: 70.00387854609929%\n",
            "Epoch [2/50], Loss: 16.26834431750287, Accuracy: 83.5017730496454%\n",
            "Epoch [3/50], Loss: 16.05271332989258, Accuracy: 85.88929521276596%\n",
            "Epoch [4/50], Loss: 16.041624964942205, Accuracy: 87.14649822695036%\n",
            "Epoch [5/50], Loss: 16.10248491769695, Accuracy: 88.05629432624114%\n",
            "Epoch [6/50], Loss: 15.891021856012214, Accuracy: 88.62311613475178%\n",
            "Epoch [7/50], Loss: 15.98930700355997, Accuracy: 89.04310726950354%\n",
            "Epoch [8/50], Loss: 15.894130099237593, Accuracy: 89.51906028368795%\n",
            "Epoch [9/50], Loss: 15.745977301502867, Accuracy: 89.74346187943263%\n",
            "Epoch [10/50], Loss: 15.611353207804166, Accuracy: 90.00221631205673%\n",
            "Epoch [11/50], Loss: 15.640237730577871, Accuracy: 90.1845079787234%\n",
            "Epoch [12/50], Loss: 15.62040218437112, Accuracy: 90.38674645390071%\n",
            "Epoch [13/50], Loss: 15.326432971026975, Accuracy: 90.48592641843972%\n",
            "Epoch [14/50], Loss: 15.547506219375451, Accuracy: 90.71919326241135%\n",
            "Epoch [15/50], Loss: 15.433497542646045, Accuracy: 90.89594414893617%\n",
            "Epoch [16/50], Loss: 15.292552792454439, Accuracy: 91.02338209219859%\n",
            "Epoch [17/50], Loss: 15.239501960155861, Accuracy: 91.08987145390071%\n",
            "Epoch [18/50], Loss: 15.024490043646567, Accuracy: 91.25941932624114%\n",
            "Epoch [19/50], Loss: 14.98038404233953, Accuracy: 91.30817819148936%\n",
            "Epoch [20/50], Loss: 15.037523610991286, Accuracy: 91.45500886524822%\n",
            "Epoch [21/50], Loss: 15.144183214017435, Accuracy: 91.53978280141844%\n",
            "Epoch [22/50], Loss: 14.719259829866319, Accuracy: 91.61347517730496%\n",
            "Epoch [23/50], Loss: 14.506122163950778, Accuracy: 91.71099290780141%\n",
            "Epoch [24/50], Loss: 14.524254724322734, Accuracy: 91.73648049645391%\n",
            "Epoch [25/50], Loss: 14.613310614307926, Accuracy: 91.74756205673758%\n",
            "Epoch [26/50], Loss: 14.622759946921477, Accuracy: 91.90769060283688%\n",
            "Epoch [27/50], Loss: 14.277524065393806, Accuracy: 91.92597517730496%\n",
            "Epoch [28/50], Loss: 14.227605474312313, Accuracy: 92.05618351063829%\n",
            "Epoch [29/50], Loss: 14.08505435931656, Accuracy: 92.11214539007092%\n",
            "Epoch [30/50], Loss: 14.353214665567716, Accuracy: 92.1736480496454%\n",
            "Epoch [31/50], Loss: 14.38921341156228, Accuracy: 92.31438386524822%\n",
            "Epoch [32/50], Loss: 14.106823993148483, Accuracy: 92.34541223404256%\n",
            "Epoch [33/50], Loss: 14.206233270438227, Accuracy: 92.39583333333333%\n",
            "Epoch [34/50], Loss: 14.130645283491523, Accuracy: 92.3864140070922%\n",
            "Epoch [35/50], Loss: 14.044917616749393, Accuracy: 92.59696365248226%\n",
            "Epoch [36/50], Loss: 13.908994229917324, Accuracy: 92.5675975177305%\n",
            "Epoch [37/50], Loss: 13.53546604358414, Accuracy: 92.61136968085107%\n",
            "Epoch [38/50], Loss: 13.76689589774086, Accuracy: 92.67453457446808%\n",
            "Epoch [39/50], Loss: 14.006273302175968, Accuracy: 92.7698359929078%\n",
            "Epoch [40/50], Loss: 13.84266195607063, Accuracy: 92.82247340425532%\n",
            "Epoch [41/50], Loss: 13.315992835861858, Accuracy: 92.95268173758865%\n",
            "Epoch [42/50], Loss: 13.66322226327785, Accuracy: 92.97096631205673%\n",
            "Epoch [43/50], Loss: 13.79331110500056, Accuracy: 92.95046542553192%\n",
            "Epoch [44/50], Loss: 13.352123150449518, Accuracy: 93.1177969858156%\n",
            "Epoch [45/50], Loss: 13.46280002774925, Accuracy: 93.09230939716312%\n",
            "Epoch [46/50], Loss: 13.75242063376644, Accuracy: 93.20644946808511%\n",
            "Epoch [47/50], Loss: 13.411681469151153, Accuracy: 93.2247340425532%\n",
            "Epoch [48/50], Loss: 13.67021852843631, Accuracy: 93.26795212765957%\n",
            "Epoch [49/50], Loss: 13.332718501619885, Accuracy: 93.3167109929078%\n",
            "Epoch [50/50], Loss: 13.424644992402616, Accuracy: 93.34275265957447%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N3TjcRQ91wtw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}